<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8" /><title></title></head><body><h1>Multiple environments and autoreset</h1>
<p>One efficient way to accelerate and improve the stability of RL is to let agents collect data in several environments at the same time. This can be done with a multiprocessing approach or a single process approach.</p>
<p>The basic idea is that, instead of interacting with a single environment, the agent can interact with a vector of environments, receiving a vector of observations and rewards and sending a vector of actions. This looks the same as performing several interaction loops over several single environments, but this can be computed faster just by leveraging the capability of python to efficiently deal with large vectors and matrices, particularly when computing with a GPU.</p>
<p>However, such use of parallel environments raises specific issues.</p>
<p>The main issue is that it is often the case that different episodes in the same environment do not last the same number of time steps. For instance, in the CartPole environment, the goal is to keep a pole up as long as possible, so the episode of a failing agent may last only a few steps (the minimum is 7) whereas a successful agent may keep it up until a predetermined time limit, e.g. 500 times steps.</p>
<p>So, if the agent is collecting data in parallel from a vector of environments, what should it do?</p>
<p>If it performs just one episode at a time for all environments, some environments will stop earlier than others and data collection with these environment will be staled until the last environment stops.  This is what the <code>autoreset=False</code> option is designed for.</p>
<p>There are contexts where it is appropriate to do this (e.g. for statistically evaluating agents) but, in most contexts, one can see this as a waste of time.</p>
<p>The other option consists in collecting data for a fixed number of steps, rather than a number of episodes and restarting (resetting) each environment each time it stops, until  the specified number of steps is reached. This way, there is no waste of time. This is what the <code>autoreset=True</code> option is designed for.</p>
<h2>The <code>autoreset=False</code> case</h2>
<p>In BBRL, GymAgents can run several environments in parallel. But when using <code>autoreset=False</code>, a stopped environment is not restarted until the other ones have all finished.</p>
<p>So, instead of specifying a number of time steps, the <code>autoreset=False</code> case can be used in conjunction with a <code>stop variable</code>: data collection stops as soon as the stop variable is <code>True</code> for all environments. The standard use case consists in using “env/done” as stop variable.</p>
<p>A specificity of the <code>autoreset=False</code> case is that once an environment is done, if other environments are not done, the data from the last step of the environment is copied into the collected data for the next time steps until the end, as illustrated below.</p>
<img src="images/noautoreset.png" alt="[copyright Sorbonne Universite]" >
<p>As shown below, this facilitates checking if all environments are done and getting the cumulated reward for each just by looking into the last time step.</p>
<img src="images/noautoreset_nenvs.png" alt="[copyright Sorbonne Universite]" >
<p>To practice about all these aspects of using a ParallelGymAgent with <code>autoreset=False</code>, you should play with <a href="02-multi_env_noautoreset.ipynb">this notebook</a>.</p>
<h2>The <code>autoreset=True</code> case</h2>
<p>The <code>autoreset=True</code> case is the case where a next episode is started as soon as an episode stops. In that case, there may be no obvious stopping variable or condition. Instead, we run the environments for n steps.</p>
<p>Collecting data for n steps offers the possibility to process this data (e.g. training the RL agent from this data) and to go back to data collection until a global training budget is elapsed. The n steps data collection part is called an epoch.</p>
<p>The collection model based on epochs faces the following list of issues:</p>
<ul>
<li>In general, the end of an epoch does not coincide with the end of all episodes. When an epochs ends before an episode stops, a specific treatment is needed to avoid loosing data from an epoch to the next. This is explained in the <a href="#binding">Binding together data collection epochs from the same episode</a> section.</li>
<li>Data from an epoch may contain several successive episodes, and the transition from an episode to the next should not be considered as a training transition, it must be properly ignored by the training process. This is explained in the <a href="#dealing">Dealing with transitions from one episode to the next</a> section.</li>
<li>An episode may stop either because the corresponding task is done, or because a time limit has been reached. This also requires a specific treatment. See <a href="time_limits.html">here</a> for more information about this issue.</li>
</ul>
<h3 id="binding">Binding together data collection epochs from the same episode</h3>
<p>When moving from a data collection epoch to the next, the transition from the last state of the previous epoch to the first state of the next epoch should not be ignored when processing the data, which is what would happen if we do nothing. This is illustrated below.</p>
<img src="images/transition_shifted_missing.png" alt=" [copyright Sorbonne Universite]" >
<p>The right way to fix this issue is to copy the last time step frame of the previous epoch into the first time step frame of the next epoch. That way, the potentially missing transition will be present in the first two frames of the second epoch. This is illustrated below.</p>
<img src="images/transition_shifted_OK.png" alt="[copyright Sorbonne Universite]" >
<p>This mechanism is implemented using <code>train_workspace.copy_n_last_steps(1)</code> as shown in <a href="">this notebook</a>.</p>
<p>Note that <code>train_workspace.zero_grad()</code> is used to clean all gradients in the workspace before processing the new data that will be collected.</p>
<h3 id="dealing"> Dealing with transitions from one episode to the next</h3>
<p>A second issue arises when the <code>autoreset=True</code> mechanism switches from episode <code>k</code> to episode <code>k+1</code> inside an epoch. The transition from the last step of episode <code>k</code> to the first step of episode <code>k+1</code> should be ignored, as it does not truly correspond to a step of the agent.</p>
<p>The appropriate way to deal with this difficulty consists in replacing single step informations with transition informations, and removing the transitions whose first step is the end of an episode and whose second step is the beginning of the second episode.</p>
<p>In practice, the sequence of steps built in the workspace is duplicated to constitute a sequence of pairs, as illustrated below. All informations of each step are duplicated: the state, the reward, the terminated, truncated and done variables, the current time and the cumulated reward.</p>
<p>Then all the transitions where the first step is done are removed. In the example below, the [step 2, step 3] transition is removed because it verifies this condition, resulting in the final list.</p>
<p>Fortunately, because of the duplication, no step information is lost (in the final list you find from step 0 to step 4), thus we can find the end of episodes by looking at the value of done in the second part of the pairs.</p>
<img src="images/transition_remove.png" alt="[copyright Sorbonne Universite]" >
<p>The function which performs this reorganization and filtering of transitions is the <code>get_transition()</code> function in the <code>Workspace</code> class.</p>
<p>Given the organization into pairs of step informations, the way to access data is a little different. To know whether an episode is done, the information is in the second part of the pair, so one should use <code>done[1]</code>. Similarly, to get V(st+1), one should use <code>critic[1]</code>. By contrast, to get the reward or the value of the critic at the current state, one should use <code>reward[0]</code> or <code>critic[0]</code>.</p>
<p>By chance, given that the pairs have two elements, <code>reward[1</code> is equivalent to <code>reward[1:]</code> (all rewards but the first) and <code>critic[1]</code> is equivalent to <code>critic[1:]</code>, thus it happens that for temporal difference updates, the code in the <code>autoreset=True</code> case is equivalent to the code in the <code>autoreset=False</code> case.</p>
<p>Note that we also have to organize the variables from the list of environments.
The list is organized as visualized below.</p>
<img src="images/transition_reorganization.png" alt="[copyright Sorbonne Universite]" >
<p>To practice about all these aspects of using a ParallelGymAgent with <code>autoreset=True</code>, you should play with <a href="03-multi_env_autoreset.ipynb">this notebook</a>.</p>
</body></html>