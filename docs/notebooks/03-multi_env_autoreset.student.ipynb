{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08628934",
   "metadata": {},
   "source": [
    "# Outlook\n",
    "\n",
    "This notebook is designed to understand how to use a gymnasium environment as a BBRL agent in practice, using autoreset=True.\n",
    "It is part of the [BBRL documentation](https://github.com/osigaud/bbrl/docs/index.html).\n",
    "\n",
    "If this is your first contact with BBRL, you may start be having a look at [this more basic notebook](01-basic_concepts.student.ipynb) and [the one using autoreset=False](02-multi_env_noautoreset.student.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb66e3c0",
   "metadata": {},
   "source": [
    "## Installation and Imports\n",
    "\n",
    "The BBRL library is [here](https://github.com/osigaud/bbrl).\n",
    "\n",
    "Below, we import standard python packages, pytorch packages and gymnasium environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f6b924",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Installs the necessary Python and system libraries\n",
    "try:\n",
    "    from easypip import easyimport, easyinstall, is_notebook\n",
    "except ModuleNotFoundError as e:\n",
    "    get_ipython().run_line_magic(\"pip\", \"install easypip\")\n",
    "    from easypip import easyimport, easyinstall, is_notebook\n",
    "\n",
    "easyinstall(\"bbrl>=0.2.2\")\n",
    "easyinstall(\"swig\")\n",
    "easyinstall(\"bbrl_gymnasium>=0.2.0\")\n",
    "easyinstall(\"bbrl_gymnasium[classic_control]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4b25ce",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "from moviepy.editor import ipython_display as video_display\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Tuple, Optional\n",
    "from functools import partial\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "import bbrl_gymnasium\n",
    "\n",
    "import copy\n",
    "from abc import abstractmethod, ABC\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from time import strftime\n",
    "OmegaConf.register_new_resolver(\n",
    "    \"current_time\", lambda: strftime(\"%Y%m%d-%H%M%S\"), replace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adee3bea",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Imports all the necessary classes and functions from BBRL\n",
    "from bbrl.agents.agent import Agent\n",
    "from bbrl import get_arguments, get_class, instantiate_class\n",
    "# The workspace is the main class in BBRL, this is where all data is collected and stored\n",
    "from bbrl.workspace import Workspace\n",
    "\n",
    "# Agents(agent1, agent2, agent3, ...) executes the different agents the one after the other\n",
    "# TemporalAgent(agent) executes an agent over multiple timesteps in the workspace, \n",
    "# or until a given condition is reached\n",
    "\n",
    "from bbrl.agents import Agents, TemporalAgent\n",
    "from bbrl.agents.gymnasium import ParallelGymAgent, make_env\n",
    "\n",
    "# Replay buffers are useful to store past transitions when training\n",
    "from bbrl.utils.replay_buffer import ReplayBuffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09be5b4d",
   "metadata": {},
   "source": [
    "## Definition of agents\n",
    "\n",
    "As before, we first create an Agent representing [the CartPole-v1 gym environment](https://gymnasium.farama.org/environments/classic_control/cart_pole/).\n",
    "This is done using the [ParallelGymAgent](https://github.com/osigaud/bbrl/blob/40fe0468feb8998e62c3cd6bb3a575fef88e256f/src/bbrl/agents/gymnasium.py#L261) class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36ec7f3",
   "metadata": {},
   "source": [
    "## Single environment case\n",
    "\n",
    "We start with a Random Agent and a single instance of the CartPole environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa60aec",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# We deal with 1 environment at a time (random seed 2139)\n",
    "\n",
    "env_agent = ParallelGymAgent(partial(make_env, env_name='CartPole-v1'), 1).seed(2139)\n",
    "obs_size, action_dim = env_agent.get_obs_and_actions_sizes()\n",
    "print(f\"Environment: observation space in R^{obs_size} and action space R^{action_dim}\")\n",
    "\n",
    "class RandomAgent(Agent):\n",
    "    def __init__(self, action_dim):\n",
    "        super().__init__()\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "    def forward(self, t: int, choose_action=True, **kwargs):\n",
    "        \"\"\"An Agent can use self.workspace\"\"\"\n",
    "        obs = self.get((\"env/env_obs\", t))\n",
    "        action = torch.randint(0, self.action_dim, (len(obs), ))\n",
    "        self.set((\"action\", t), action)\n",
    "\n",
    "# Each agent will be run (in the order given when constructing Agents)\n",
    "agents = Agents(env_agent, RandomAgent(action_dim))\n",
    "t_agents = TemporalAgent(agents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfacd9f1",
   "metadata": {},
   "source": [
    "Let us have a closer look at the content of the workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fd0774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a new workspace\n",
    "workspace = Workspace() \n",
    "t_agents(workspace, stop_variable=\"env/done\")\n",
    "\n",
    "# We get the transitions: each tensor is transformed so\n",
    "# that: \n",
    "# - we have the value at time step t and t+1 (so all the tensors first dimension have a size of 2)\n",
    "# - there is no distinction between the different environments (here, there is just one environment run in parallel to make it easy)\n",
    "transitions = workspace.get_transitions()\n",
    "\n",
    "# You can see that each pair of actions in the transitions can be found in the workspace\n",
    "display(\"Observations (first 3)\", workspace[\"env/env_obs\"][:3, 0])\n",
    "\n",
    "display(\"Transitions of actions (first 3)\")\n",
    "for t in range(3):\n",
    "    display(f'(s_{t}, s_{t+1})')\n",
    "    display(transitions[\"env/env_obs\"][:, t])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc466ff",
   "metadata": {},
   "source": [
    "## Multiple environment case\n",
    "\n",
    "Now we are using 3 environments.\n",
    "Given the organization of transitions, to find the transitions of a particular environment\n",
    "we have to watch in the transition every 3 lines, since transitions are stored one environment after the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e494fc",
   "metadata": {},
   "source": [
    "## The replay buffer\n",
    "\n",
    "Differently from the previous case, we use a replace buffer that stores\n",
    "a set of transitions $(s_t, a_t, r_t, s_{t+1})$\n",
    "Finally, the replay buffer keeps slices [:, i, ...] of the transition\n",
    "workspace (here at most 100 transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd66a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "rb = ReplayBuffer(max_size=100)\n",
    "\n",
    "# We add the transitions to the buffer....\n",
    "rb.put(transitions)\n",
    "\n",
    "# And sample from them here we get 3 tuples (s_t, s_{t+1})\n",
    "rb.get_shuffled(3)[\"env/env_obs\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cadbd34",
   "metadata": {},
   "source": [
    "A transition workspace is still a workspace... this is quite\n",
    " handy since each transition can be seen as a mini-episode of two time steps;\n",
    " we can use our agents on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed16ee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just as a reference\n",
    "\n",
    "display(transitions[\"action\"])\n",
    "\n",
    "t_random_agent = TemporalAgent(RandomAgent(action_dim))\n",
    "t_random_agent(transitions, t=0, n_steps=2)\n",
    "\n",
    "# Here, the action tensor will have been overwritten by the new actions\n",
    "display(transitions[\"action\"])"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
